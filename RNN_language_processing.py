#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: William Collier
"""
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt


#%%

"""
Natural language processing with RNN
Recurrent neural net
Can be used for autocomplete or spellcheck
"""

# working towards senitment analysis
# Take movie reviews, and find out if something is poitive or negative
# Then charcter generation -> generator the next charcter in a sequence of 
# text, to generate an entire play



# Compare the differences between text and numerical data
# Because the neural net still needs numeric data

# Bag of words --> very famous text to numeric, but floored and only for
# simple tasks

# Every unique word in the dataset needs its own integer 
# These must be stored in a dictionary
# KNown as a word lookup table
# Doesn't care about the order of the words
# Therefore two sentences which mean different things can use the same words
# And would be confused by the network
   # ------------------------- #

# Bag of words style, but leave the integers in order 
# For 100,000 words, need 100,000 mappings
# But if words are similar, but the numbers are significantly different
# Hard to train a net to recognise this
# I.e. happy: 1, good: 100,000 but 2:sad difference 1-2 is more significant 
# than for 999,999

# -------------------------- #

# Word embedding 
#  translate each word into a vector of n dimensions
# these describe how similar the words are to each other
# hope that the vectors have enough dimensions that the angles
# between similar words have small angles, and different words 
# are large angles

# Add a layer to do the word embedding
# these embeddings are therefore learnt
# tries to do this with "context"

# --------------------------- #

# Recurrent neural nets 
# fundamental difference between dense nn or conv NN is a contained loop
#  trains in time stes and builds up upon things it has already learnt
# (CNN or Dense NN are feed-forward networks, all info goes left to right)
# RNN feeds one word at a time, and analyses it then moves forward
# to treat it like a human reading a sentence, 
# developing an understanding of context


# LSTM -> long-short term memory
# adds a component that tracks the internal state
# not just the previous output for the last input

from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
import os

def plot_graphs(history, metric):
    plt.plot(history.history[metric])
    plt.plot(history.history['val_'+metric], '')
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend([metric, 'val_'+metric])
    plt.how()
    
    
VOCAB_SIZE = 88584

MAXLEN = 250
BATCH_SIZE = 64

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)

# data is encoded into integers, and the integers are generated by how common 
# the word is -> integer 1 is the most common word

# 

# print(train_data[0])

# If the review is >250 words then trim extra words
# If review is <250 words add 0's until the length is 250
# The 0's are added from position 0 in the array

train_data = sequence.pad_sequences(train_data, MAXLEN)
test_data = sequence.pad_sequences(test_data, MAXLEN)
# 

# Dense acitvation sigmoid, predict the sentiment, 0-->1 bad to good
# Embedding to make the vectors to replace the integer values
# 32 dimensions in the LSTM layer and as teh output from Embedding
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(VOCAB_SIZE, 32),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(1, activation="sigmoid")    
    ])

model.summary()

# 

model.compile(loss='binary_crossentropy', optimizer="rmsprop",metrics=['acc'])

history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)
# Can be seen that val_acc stalls after only a few epochs
# Therefore not enough training data, likely overfit as acc continues to increase

# 
# Test the accuracy
results = model.evaluate(test_data, test_labels)
print(results)

# 
# Make predictions

# This is the lookup table that gives the mappings from words to integers
word_index = imdb.get_word_index()

# Need to convert any future data into the form used by the network previously 
def encode_text(text):
    # given a text, convert the text into tokens
    tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)
    # if the worse in tokens is in the mapping, replace it's location with the 
    # specific integer
    # otherwise replace with zero
    tokens = [word_index[word] if word in word_index else 0 for word in tokens]
    # pad the sequence 
    return sequence.pad_sequences([tokens], MAXLEN)[0]


text = "that movie was just amazing, so amazing"
encoded = encode_text(text)
print(encoded)


# 

# This will make a decoding function, to turn from integer into words
reverse_word_index = {value: key for (key, value) in word_index.items()}

def decode_integers(integers):
    PAD = 0
    text = ""
    for num in integers:
        if num != PAD:
            text += reverse_word_index[num] + " "
    
    return text[:-1]

print(decode_integers(encoded))

# 

# now to make a prediction

def predict(text):
    # first encode the text
    encoded_text = encode_text(text)
    # blank array to match the shape expected
    pred = np.zeros((1, 250))
    # add the encoded text
    pred[0] = encoded_text
    # predict the array
    result = model.predict(pred)
    if result[0] > 0.5: print("Positive")
    else: print("negative")
    print(result[0])
    
    
positive_review = "That movie was so awesome! I really loved it and would watch it again because it is great."
predict(positive_review)

negative_review = "That movie so bad. I hated it and wouldn't watch it again. One of the worst things I've watched."
predict(negative_review)

# But reordering the sentences, or adding words can change these predictions
# therefore needs to be more robust if it was used in practise
# or at least more training sample
